<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>main</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p><strong>Automatic Image Filter Identification</strong> <strong>Vision 20/20</strong><br />
CS4476, Project Proposal Gokmen, Dumenci, Ye, Lo<br />
Prof. Parikh Georgia Institute of Technology</p>
<h1 id="problem-statement" class="unnumbered unnumbered">Problem Statement</h1>
<p>With the prevalence of Instagram and related social networks, filters on images are more common than ever. We attempt to build a filter identification system so viewers can be more aware of how the contents they consume are edited. Our proposed system will take a filtered image and identify what filter has been applied. We currently constrain considered filters to a simplified set that perform global (position invariant) filtering. These filters can change color, brightness, contrast, etc. and do not include rotation, shear, and transformation. Immediate extensions include expanding the filters set to include vignette effects, which vary based on location in the image, and training the system to identify the type of film camera used. A separate but related problem is to create a system for filter inversion, outputting a raw image based on a filtered image and known filter, which will be considered, time permitting.</p>
<h1 id="approach" class="unnumbered unnumbered">Approach</h1>
<h2 id="learning-end-to-end-using-a-cnn" class="unnumbered unnumbered">Learning End-to-End using a CNN</h2>
<p>For this project, we will try two approaches in parallel. The first, ‘brute force’ approach will involve the use of a Convolutional Neural Network. The CNN will take a filtered image as an input and output the hypothesized filter. <span class="math display">\[f(I) : \text{Image} \to \text{Filter}\]</span> We will explore various proven CNN architectures such as AlexNet, VGG, and ResNet, and compare their performance in filter identification. Because CNNs are designed for picking up image structure, which is not necessarily relevant to filter identification, various preprocessing steps such as blurring will be tested to attempt to improve the results. We believe that there’s a possibility for the CNN to learn non-goal features such as image structure, and perform poorly on global properties such as filters applied. Therefore, we will experiment with an alternate method in parallel, following a more structured approach designed to imitate human filter perception.</p>
<h2 id="learning-on-preselected-features" class="unnumbered unnumbered">Learning on Preselected Features</h2>
<p>When a trained eye considers a scene with a forest, it normally expects colors in the greens and browns. When a filter is applied to this scene, we hypothesize that the shift in these expected colors allows the recognition of the filter. Then, imitating human perception, given a filtered image we want to recognize its scene and compute metrics, and run it through a classifier that returns the applied filter.</p>
<p>Formally, we define <span class="math inline">\(y = m(x)\)</span> to be a function from an image <span class="math inline">\(x\)</span> to a vector in <span class="math inline">\(R^n\)</span> where <span class="math inline">\(n\)</span> is the number of metrics we calculate. Each entry <span class="math inline">\(y_i\)</span> is the result of the calculation of the <span class="math inline">\(i\)</span>-th metric on the image <span class="math inline">\(x\)</span>. Similarly, we define <span class="math inline">\(z = s(x)\)</span> to be a function from an image <span class="math inline">\(x\)</span> to a vector in <span class="math inline">\(R^k\)</span> where <span class="math inline">\(k\)</span> is the total number of distinct scenes we are capable of identifying. Each element <span class="math inline">\(z_i\)</span> is the probability that the image <span class="math inline">\(x\)</span> is that of the scene <span class="math inline">\(i\)</span>, therefore <span class="math inline">\(z\)</span> is a probability vector where each element is nonnegative and the elements sum to <span class="math inline">\(1\)</span>.</p>
<p>Then, our target function <span class="math inline">\(g\)</span> is a classifier that, given the metrics and the scene probability vector for an image, outputs a filter probability vector:</p>
<p><span class="math display">\[g(m(x), s(x)) : (\text{Metrics},\  \text{Scene}) \to \text{Filter}\]</span></p>
<p>For every training image vector <span class="math inline">\(x\)</span> known to be in scene <span class="math inline">\(c\)</span> and the result of filter <span class="math inline">\(y\)</span>, we compute the feature vector <span class="math inline">\(m\)</span> containing our predetermined image metrics, and the scene vector <span class="math inline">\(s\)</span> where <span class="math inline">\(s_c = 1\)</span> and the rest is zero. We store this instance <span class="math inline">\(g(m, s) = y\)</span> in our knowledge base. We then train our model for <span class="math inline">\(g\)</span>, likely a Decision Tree classifier, on this knowledge base.</p>
<p>Then, to use the classifier on an image vector <span class="math inline">\(x&#39;\)</span>, we compute the scene probability vector <span class="math inline">\(s&#39;\)</span> using an AlexNet trained on Places <span class="citation" data-cites="Places"></span> or a similar classifier, and the metrics vector <span class="math inline">\(m&#39;\)</span>. We then input the pair <span class="math inline">\(m&#39;, s&#39;\)</span> to our model for <span class="math inline">\(g\)</span> to guess the applicable filter for the image.</p>
<h3 id="improvements" class="unnumbered unnumbered">Improvements</h3>
<p>An immediate improvement to the feature approach stems from the concern that metrics can have high variance even within scenes, introducing ambiguity to the classification process.</p>
<p>To help resolve this, we can use an object detector (YOLOv2, MobileNet, ...) and tag objects in the scene. We can then treat these objects as their own images, and compute metrics for each object. This would be done within the context of the scene. (A teapot in a room is a different entity than a teapot in a forest.) As a result, each entity will have its own predictions for which filter was used locally, contributing to the decision of the filter used for the overall image. We could have a uniform vote based on most likely filter per entity, or we could combine entities’ confidences to produce image level confidences.</p>
<p>Another possible avenue of improvement involves the selection of metrics. The metrics we picked can be analyzed using statistical methods to reduce the number of features. Methods such as autoencoding or principal component analysis can be implemented to combine and eliminate similar metrics such that only those providing the most information would be used for the main classifier.</p>
<h1 id="experiments-and-results" class="unnumbered unnumbered">Experiments and Results</h1>
<h2 id="dataset" class="unnumbered unnumbered">Dataset</h2>
<p>The aforementioned methods require large amounts of data for training, as a result of filters having identifiable characteristics only when certain knowledge about the prior image is known. Examples of such knowledge depend on the identification metric used, and can include color or brightness histograms, gradient matrices.</p>
<p>As the source of this data, we intend to use Yahoo’s YFCC100M dataset <span class="citation" data-cites="Yahoo"></span> consisting of 100M images. We will apply the considered filters to these images, generating pairs of filter type and filtered image. This dataset will be then be used in both the CNN method and the structured method in learning filter identification.</p>
<h2 id="implementation" class="unnumbered unnumbered">Implementation</h2>
<p>For ease of development, we will utilize Python and its various libraries for image processing and manipulation and for machine learning algorithms. We will use skimage for our image processing and feature extraction, sklearn for non neural network based machine learning algorithms, and pytorch for implementing and testing our neural network architectures. We will also leverage existing libraries such as sklearn for validation and testing.</p>
<h2 id="filters" class="unnumbered unnumbered">Filters</h2>
<p>Without loss of generality, we will select 5 filters based on a study by Canva on the most used Instagram filters <span class="citation" data-cites="Canva"></span> which are the following:</p>
<ul>
<li><p>Clarendon</p></li>
<li><p>Gingham</p></li>
<li><p>Juno</p></li>
<li><p>Lark</p></li>
<li><p>Mayfair</p></li>
</ul>
<p>Luckily, these filters does not introduce any vignetting effects and are all global filters and thus are suited to our purposes. Other filters may be added during the later stages of the study.</p>
<h2 id="experiments" class="unnumbered unnumbered">Experiments</h2>
<p>Our initial experimentation would limit both the number of filters we try to predict and the variation in these images to see if good results are possible under these constrained conditions. We will first evaluate each approach first based on a limited set of filters and limited set of images containing similar color distributions and then expand the existing methods to classify multiple filters. Since it is possible that color distributions will not be indicative of the result of filters, we will also limit our training images to be of similar scene and context. The progression of experimentation on applying both NN methods and heuristic methods will be based on the following:</p>
<ul>
<li><p>Applying a single filter to training images of similar scenes/color distributions and test our methods in classifying filter type</p></li>
<li><p>Expand training images to include a variety of scene context and color distribution while applying a single filter</p></li>
<li><p>Expand filter set to multiple filters and test performance</p></li>
<li><p>Expand and test our heuristic methods to see if we can improve performance</p></li>
</ul>
<p>We consider each step to be success after reaching <span class="math inline">\(\geq 90\%\)</span> accuracy on classifying image filters using the respective classification methods. We will also pay attention to various other metrics such as precision and recall to see how our models are performing in each category.</p>
<p>These experiments should reveal how effective various neural network architectures are in detecting global image filter information and the “naturalness” of an image. They should also reveal whether filters in general are distinguishable from real photos and how good are heuristic measures are at detecting that difference. It is certainly uncertain whether heuristic based methods or CNNs are effective at capturing &quot;naturalness&quot; information and these experiments will certainly reveal whether more carefully crafted feature extractions and preprocessing are required to obtain good results.</p>
<h1 id="extension-filter-inversion" class="unnumbered unnumbered">Extension: Filter Inversion</h1>
<p>An extension to the project upon successful completion of the primary goals is the inversion of identified filters. While the filters involved in the project are mostly functions on the pixel values individually, because the set of possible pixel values is a discrete, finite set, most of our filters involve rounding/clipping. As a result, the underlying functions are not one-to-one and thus their inversion might cause similar but distinct colors in the original image to map to identical colors in the filter-inverted image. We will apply filtering methods to correct this and to make the image look as much like the original while trying to mitigate resolution loss at the same time. Coupled with the principle goals of the project, the aim of the extension would be to produce the unfiltered original of any given input image that is the result of the application of a single filter to an unfiltered image.</p>
<p><span>9</span> B. Thomee, D.A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, L. Li. <em>YFCC100M: The New Data in Multimedia Research</em>. Communications of the ACM, 59(2), pp. 64-73, 2016.</p>
<p>Canva. <em>Popular Instagram Filters</em>. https://www.canva.com/learn/popular-instagram-filters/</p>
<p>B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba <em>Places: A 10 million Image Database for Scene Recognition</em>. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.</p>
</body>
</html>
